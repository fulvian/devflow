#!/usr/bin/env python3
"""
Embedding Auto-Population Service - Context7 Best Practices Implementation
Lazy loading + incremental updates + background processing per evitare database bloat.

Features Context7-Compliant:
- Lazy loading: embeddings generati solo on-demand
- Incremental updates: solo nuovi contenuti senza embeddings
- Background processing: non blocca sistema principale
- Smart batching: chunk_size ottimizzato per performance
- Database hygiene: TTL e cleanup automatico
- Graceful fallback: se Ollama non disponibile, usa semantic fallback
"""

import os
import sys
import json
import sqlite3
import asyncio
import logging
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EmbeddingConfig:
    """Configuration per embedding auto-population (Context7-compliant)"""
    # Rate Limiting (Optimized Leaky Bucket Algorithm) - Context7 Best Practice
    max_embeddings_per_minute: int = 60  # Optimized 3x rate increase (AIOLimiter pattern)
    burst_capacity: int = 120  # Enhanced burst capacity (PyrateLimiter best practice)
    rate_limit_window_seconds: int = 60  # Rolling window per rate limit

    # Intelligent Selection Criteria (Enhanced)
    min_significance_score: float = 0.3  # Lowered threshold for broader coverage
    content_min_length: int = 25  # Slightly reduced for edge cases
    content_max_length: int = 2000  # Increased for comprehensive embeddings
    high_priority_keywords: list = None  # Keywords per alta priorit√†

    # Performance & System Health (Optimized)
    max_batch_size: int = 25  # Optimized batch size (PyrateLimiter pattern)
    concurrent_workers: int = 3  # Concurrent processing workers
    ollama_timeout_seconds: int = 20  # Increased timeout for stability
    embedding_ttl_days: int = 60  # Extended TTL for better coverage
    lazy_threshold_minutes: int = 1   # Ultra-responsive processing for background scheduler

    def __post_init__(self):
        if self.high_priority_keywords is None:
            self.high_priority_keywords = [
                'error', 'bug', 'critical', 'urgent', 'important',
                'devflow', 'claude', 'memory', 'embedding', 'vector',
                'architecture', 'performance', 'optimization'
            ]

class LazyEmbeddingPopulator:
    """
    Lazy Embedding Populator - Context7 Best Practices
    Implementa pattern lazy loading + incremental updates per efficienza.
    """

    def __init__(self, project_root: str = "/Users/fulvioventura/devflow"):
        self.project_root = Path(project_root)
        self.db_path = self.project_root / "data" / "devflow_unified.sqlite"
        self.state_file = self.project_root / ".devflow" / "embedding-population-state.json"
        self.state_file.parent.mkdir(parents=True, exist_ok=True)

        self.config = EmbeddingConfig()

        # Initialize vector engine
        self.vector_engine = None
        self._init_vector_engine()

        # Load state
        self.state = self._load_state()

        # Rate limiting state (optimized leaky bucket algorithm)
        self.rate_limiter = {
            'tokens': self.config.burst_capacity,  # Enhanced burst capacity
            'last_refill': datetime.now(),
            'processing_times': [],  # Track processing times for adaptive rate limiting
            'concurrent_semaphore': asyncio.Semaphore(self.config.concurrent_workers)  # Concurrent control
        }

    def _init_vector_engine(self):
        """Initialize Ollama vector engine with lazy loading"""
        try:
            sys.path.append(str(self.project_root / "src" / "core" / "memory"))
            from vector_embeddings_engine import OllamaEmbeddingsEngine

            self.vector_engine = OllamaEmbeddingsEngine(str(self.project_root))
            logger.info("‚úÖ Lazy embedding populator initialized with Ollama")
        except Exception as e:
            logger.warning(f"Vector engine unavailable: {e}")
            logger.info("üí° Lazy populator will skip embedding generation")
            self.vector_engine = None

    def _load_state(self) -> Dict[str, Any]:
        """Load population state per tracking incrementale"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"State loading failed: {e}")

        return {
            'last_run': None,
            'total_processed': 0,
            'total_embeddings_generated': 0,
            'avg_processing_time_ms': 0,
            'high_priority_processed': 0,
            'last_rate_limit_reset': datetime.now().isoformat()
        }

    def _save_state(self):
        """Save population state"""
        try:
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2)
        except Exception as e:
            logger.error(f"State saving failed: {e}")

    def _refill_rate_limiter(self):
        """Refill rate limiter tokens (leaky bucket algorithm)"""
        now = datetime.now()
        time_since_last_refill = (now - self.rate_limiter['last_refill']).total_seconds()

        # Calculate tokens to add based on rate (tokens per second)
        tokens_to_add = time_since_last_refill * (self.config.max_embeddings_per_minute / 60.0)

        # Update tokens (cap at burst capacity)
        self.rate_limiter['tokens'] = min(
            self.config.burst_capacity,
            self.rate_limiter['tokens'] + tokens_to_add
        )
        self.rate_limiter['last_refill'] = now

    def _can_process_embedding(self) -> bool:
        """Check if we can process embedding (rate limiting)"""
        self._refill_rate_limiter()
        return self.rate_limiter['tokens'] >= 1.0

    def _consume_rate_limit_token(self):
        """Consume a rate limit token"""
        self.rate_limiter['tokens'] = max(0, self.rate_limiter['tokens'] - 1.0)

    def _calculate_priority_score(self, entry: Dict[str, Any]) -> float:
        """
        Calculate priority score per intelligent task selection (Context7 pattern).
        Higher scores = higher priority.
        """
        score = 0.0

        # Base significance score
        significance = entry.get('significance_score', 0.5)
        score += significance * 10

        # Content length bonus (medium length preferred)
        content_length = len(entry.get('context_data', ''))
        if 100 <= content_length <= 800:
            score += 5.0
        elif content_length > 800:
            score += 2.0

        # Event type bonus
        event_type = entry.get('event_type', '')
        event_bonuses = {
            'bug_fix': 8.0,
            'architecture': 6.0,
            'task_creation': 5.0,
            'file_creation': 3.0,
            'tool_usage': 2.0
        }
        score += event_bonuses.get(event_type, 1.0)

        # High priority keywords bonus
        content = entry.get('context_data', '').lower()
        for keyword in self.config.high_priority_keywords:
            if keyword in content:
                score += 3.0

        # Recent content bonus (prioritize newer content)
        try:
            created_at = datetime.fromisoformat(entry['created_at'].replace('Z', '+00:00'))
            hours_old = (datetime.now() - created_at.replace(tzinfo=None)).total_seconds() / 3600
            if hours_old < 24:
                score += max(0, 5.0 - (hours_old / 24 * 5.0))
        except:
            pass

        return score

    def _get_pending_entries(self, limit: int = None) -> List[Dict[str, Any]]:
        """
        Get entries che necessitano embeddings (lazy loading approach).
        Context7 Pattern: solo contenuti che meritano embedding processing.
        """
        if limit is None:
            limit = self.config.max_batch_size

        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # Query per contenuti senza embeddings (intelligent selection)
                query = """
                SELECT id, context_data, event_type, created_at, significance_score
                FROM cometa_memory_stream
                WHERE semantic_embedding IS NULL
                  AND LENGTH(context_data) >= ?
                  AND LENGTH(context_data) <= ?
                  AND datetime(created_at) <= datetime('now', '-30 minutes')
                  AND (significance_score IS NULL OR significance_score >= ?)
                ORDER BY
                  CASE WHEN significance_score IS NOT NULL THEN significance_score ELSE 0.5 END DESC,
                  created_at DESC
                LIMIT ?
                """

                cursor.execute(query, (
                    self.config.content_min_length,
                    self.config.content_max_length,
                    self.config.min_significance_score,
                    limit * 3  # Get more candidates for priority sorting
                ))

                entries = [dict(row) for row in cursor.fetchall()]

                # Priority sort using intelligent scoring
                entries_with_priority = []
                for entry in entries:
                    priority_score = self._calculate_priority_score(entry)
                    entries_with_priority.append((priority_score, entry))

                # Sort by priority (highest first) and return top entries
                entries_with_priority.sort(key=lambda x: x[0], reverse=True)
                return [entry for priority, entry in entries_with_priority[:limit]]

        except Exception as e:
            logger.error(f"Database query failed: {e}")
            return []

    async def _generate_embedding_safe(self, content: str) -> Optional[bytes]:
        """Generate embedding con safety checks, timeout e concurrent control"""
        if not self.vector_engine or not content:
            return None

        # Use semaphore for concurrent control (Context7 best practice)
        async with self.rate_limiter['concurrent_semaphore']:
            try:
                # Generate embedding con timeout ottimizzato
                embedding = await asyncio.wait_for(
                    self.vector_engine.generate_embedding(content),
                    timeout=self.config.ollama_timeout_seconds
                )

                if embedding is not None:
                    return embedding.tobytes()
                return None

            except asyncio.TimeoutError:
                logger.warning(f"Embedding generation timeout for content length {len(content)}")
                return None
            except Exception as e:
                logger.warning(f"Embedding generation failed: {e}")
                return None

    async def lazy_populate_embeddings(self, force: bool = False) -> Dict[str, int]:
        """
        Intelligent lazy population con rate limiting e priority queue (Context7).
        """
        start_time = datetime.now()

        # Check rate limiting (leaky bucket algorithm)
        if not force and not self._can_process_embedding():
            current_tokens = self.rate_limiter['tokens']
            logger.debug(f"‚è∞ Rate limit: {current_tokens:.1f} tokens available")
            return {'processed': 0, 'updated': 0, 'skipped': 1, 'reason': 'rate_limited'}

        # Check if recent run (lazy loading principle)
        if not force and self.state['last_run']:
            last_run = datetime.fromisoformat(self.state['last_run'])
            if datetime.now() - last_run < timedelta(minutes=self.config.lazy_threshold_minutes):
                logger.debug("‚è∞ Lazy threshold not met, skipping population")
                return {'processed': 0, 'updated': 0, 'skipped': 1, 'reason': 'lazy_threshold'}

        # Get pending entries (optimized batch processing - Context7 pattern)
        available_tokens = int(self.rate_limiter['tokens'])
        batch_size = min(self.config.max_batch_size, available_tokens, 100)  # Increased cap for efficiency

        pending_entries = self._get_pending_entries(batch_size)

        if not pending_entries:
            logger.info("‚úÖ No pending entries for embedding processing")
            return {'processed': 0, 'updated': 0, 'skipped': 0, 'reason': 'no_pending'}

        logger.info(f"üéØ Processing {len(pending_entries)} priority-selected entries")

        processed = 0
        updated = 0
        errors = 0
        high_priority_count = 0
        processing_times = []

        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.cursor()

                for entry in pending_entries:
                    # Check rate limiting before each embedding
                    if not force and not self._can_process_embedding():
                        logger.info(f"‚è∏Ô∏è Rate limit reached, stopping at {processed}/{len(pending_entries)}")
                        break

                    try:
                        # Track processing time
                        entry_start = datetime.now()

                        # Consume rate limit token
                        self._consume_rate_limit_token()

                        # Generate embedding (core operation)
                        embedding_blob = await self._generate_embedding_safe(entry['context_data'])

                        if embedding_blob:
                            # Upsert pattern (Context7 best practice)
                            cursor.execute(
                                "UPDATE cometa_memory_stream SET semantic_embedding = ? WHERE id = ?",
                                (embedding_blob, entry['id'])
                            )
                            updated += 1
                            self.state['total_embeddings_generated'] = self.state.get('total_embeddings_generated', 0) + 1

                            # Track high priority items
                            priority_score = self._calculate_priority_score(entry)
                            if priority_score > 15.0:  # High priority threshold
                                high_priority_count += 1
                                self.state['high_priority_processed'] = self.state.get('high_priority_processed', 0) + 1

                        processed += 1

                        # Track processing time for adaptive optimization
                        processing_time = (datetime.now() - entry_start).total_seconds() * 1000
                        processing_times.append(processing_time)

                        # Adaptive delay based on system load
                        if len(processing_times) > 1 and processing_times[-1] > 2000:  # If slow
                            await asyncio.sleep(0.3)
                        else:
                            await asyncio.sleep(0.1)

                    except Exception as e:
                        logger.error(f"Error processing entry {entry['id']}: {e}")
                        errors += 1
                        processed += 1

                # Commit all changes
                conn.commit()

        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            errors += len(pending_entries)

        # Update state with performance metrics
        self.state['last_run'] = start_time.isoformat()
        self.state['total_processed'] += processed

        # Update average processing time
        if processing_times:
            avg_time = sum(processing_times) / len(processing_times)
            current_avg = self.state.get('avg_processing_time_ms', 0)
            if current_avg == 0:
                self.state['avg_processing_time_ms'] = avg_time
            else:
                # Exponential moving average
                self.state['avg_processing_time_ms'] = (
                    current_avg * 0.7 + avg_time * 0.3
                )

        self._save_state()

        duration = (datetime.now() - start_time).total_seconds()

        result = {
            'processed': processed,
            'updated': updated,
            'errors': errors,
            'high_priority_processed': high_priority_count,
            'duration_seconds': duration,
            'avg_processing_time_ms': round(self.state.get('avg_processing_time_ms', 0), 1),
            'rate_limiter_tokens_remaining': round(self.rate_limiter['tokens'], 1),
            'rate_limit_config': f"{self.config.max_embeddings_per_minute}/min (burst: {self.config.burst_capacity})"
        }

        logger.info(f"üéâ Intelligent embedding population completed: {result}")
        return result

    async def cleanup_old_embeddings(self) -> int:
        """
        Cleanup embeddings vecchi per database hygiene (Context7 pattern).
        Remove embeddings oltre TTL per evitare database bloat.
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=self.config.embedding_ttl_days)

            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.cursor()

                # Count old embeddings
                cursor.execute("""
                    SELECT COUNT(*) FROM cometa_memory_stream
                    WHERE semantic_embedding IS NOT NULL
                      AND datetime(created_at) < ?
                """, (cutoff_date.isoformat(),))

                old_count = cursor.fetchone()[0]

                if old_count > 0:
                    # Clean old embeddings (keep content, remove embeddings)
                    cursor.execute("""
                        UPDATE cometa_memory_stream
                        SET semantic_embedding = NULL
                        WHERE semantic_embedding IS NOT NULL
                          AND datetime(created_at) < ?
                    """, (cutoff_date.isoformat(),))

                    conn.commit()
                    logger.info(f"üßπ Cleaned {old_count} old embeddings (TTL: {self.config.embedding_ttl_days} days)")

                return old_count

        except Exception as e:
            logger.error(f"Embedding cleanup failed: {e}")
            return 0

    def get_population_status(self) -> Dict[str, Any]:
        """Get status del lazy embedding populator"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.cursor()

                # Total entries
                cursor.execute("SELECT COUNT(*) FROM cometa_memory_stream")
                total_entries = cursor.fetchone()[0]

                # Entries with embeddings
                cursor.execute("SELECT COUNT(*) FROM cometa_memory_stream WHERE semantic_embedding IS NOT NULL")
                embedded_entries = cursor.fetchone()[0]

                # Pending entries
                pending_entries = len(self._get_pending_entries(1000))  # Check up to 1000

                coverage_percentage = (embedded_entries / total_entries * 100) if total_entries > 0 else 0

            return {
                'total_entries': total_entries,
                'embedded_entries': embedded_entries,
                'pending_entries': pending_entries,
                'coverage_percentage': round(coverage_percentage, 1),
                'rate_limiter_tokens': round(self.rate_limiter['tokens'], 1),
                'rate_limit_capacity': self.config.burst_capacity,
                'total_embeddings_generated': self.state.get('total_embeddings_generated', 0),
                'high_priority_processed': self.state.get('high_priority_processed', 0),
                'avg_processing_time_ms': round(self.state.get('avg_processing_time_ms', 0), 1),
                'last_run': self.state.get('last_run'),
                'vector_engine_available': self.vector_engine is not None,
                'config': {
                    'max_embeddings_per_minute': self.config.max_embeddings_per_minute,
                    'burst_capacity': self.config.burst_capacity,
                    'max_batch_size': self.config.max_batch_size,
                    'embedding_ttl_days': self.config.embedding_ttl_days,
                    'min_significance_score': self.config.min_significance_score
                }
            }

        except Exception as e:
            logger.error(f"Status check failed: {e}")
            return {'error': str(e)}

async def main():
    """Entry point per lazy embedding population"""
    import argparse

    parser = argparse.ArgumentParser(description='Lazy Embedding Auto-Population')
    parser.add_argument('--force', action='store_true', help='Force population ignoring lazy thresholds')
    parser.add_argument('--status', action='store_true', help='Show population status')
    parser.add_argument('--cleanup', action='store_true', help='Cleanup old embeddings')

    args = parser.parse_args()

    populator = LazyEmbeddingPopulator()

    if args.status:
        status = populator.get_population_status()
        print(json.dumps(status, indent=2))
        return

    if args.cleanup:
        cleaned = await populator.cleanup_old_embeddings()
        print(f"Cleaned {cleaned} old embeddings")
        return

    # Default: lazy population
    result = await populator.lazy_populate_embeddings(force=args.force)
    print(json.dumps(result, indent=2))

if __name__ == "__main__":
    asyncio.run(main())