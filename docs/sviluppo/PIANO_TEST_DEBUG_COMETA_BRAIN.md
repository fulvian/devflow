# Piano di Test e Debug - Cometa Brain v2.0 Sistema Cognitivo Completo

## Executive Summary
Piano completo per testing, debugging e deployment del **Sistema Cometa Brain v2.0**, verificando conformit√† alle specifiche architetturali originali e KPI di business.

**Targets di Conformit√† Originale**:
- **Authority Centralization**: 100% task override da Claude Code ‚úì
- **4-Layer Intelligence**: Tutti i livelli operativi e verificati ‚úì
- **Task Auto-Creation Rate**: >80% (Target: 85%) ‚úì
- **Context Relevance Score**: >85% (Target: 90%) ‚úì
- **Session Continuity**: <5 secondi ricostruzione contesto ‚úì
- **Hook Performance**: <500ms execution time (Target: <300ms) ‚úì
- **Coverage**: 95% | **Latency**: <100ms p95 | **Reliability**: 99.9%

---

## 1. ARCHITETTURA DI TEST

### 1.1 Stack Tecnologico
```yaml
unit_testing:
  framework: pytest
  coverage: pytest-cov
  mocking: pytest-mock, monkeypatch
  fixtures: pytest fixtures

integration_testing:
  database: pytest-postgresql
  async: pytest-asyncio
  concurrency: pytest-xdist

e2e_testing:
  framework: playwright-python
  browsers: [chromium, firefox, webkit]
  api_testing: pytest-httpx

performance:
  profiling: pytest-benchmark
  memory: pytest-memray
  line_profiling: pytest-line-profiler

ci_cd:
  pipeline: GitHub Actions
  quality_gates: pre-commit hooks
  reporting: pytest-html
  monitoring: opentelemetry
```

### 1.2 Test Pyramid
```
         /\
        /E2E\         (5%)  - Critical user journeys
       /-----\
      /  INT  \       (15%) - Component integration
     /---------\
    /   UNIT    \     (70%) - Component logic
   /-------------\
  / STATIC CHECKS \   (10%) - Linting, type checking
 /-----------------\
```

---

## 2. UNIT TESTING

### 2.1 NLP Processor Tests
```python
# tests/unit/test_nlp_processor.py

import pytest
from pathlib import Path
from unittest.mock import Mock, patch
from cometa_nlp_processor import NaturalLanguageCommandProcessor

class TestNLPProcessor:
    """Test suite for Natural Language Command Processor"""

    @pytest.fixture
    def processor(self, tmp_path):
        """Create processor with test database"""
        test_db = tmp_path / "test.db"
        return NaturalLanguageCommandProcessor(test_db)

    @pytest.fixture
    def mock_db_connection(self, monkeypatch):
        """Mock database connection"""
        mock_conn = Mock()
        mock_cursor = Mock()
        mock_conn.cursor.return_value = mock_cursor
        monkeypatch.setattr('sqlite3.connect', Mock(return_value=mock_conn))
        return mock_conn, mock_cursor

    # Intent Detection Tests
    @pytest.mark.parametrize("input_text,expected_intent", [
        ("create task for user authentication", "task_creation"),
        ("list all active tasks", "task_listing"),
        ("complete task AUTH-001", "task_completion"),
        ("update task priority to high", "task_update"),
        ("search tasks about API", "task_search"),
        ("delete old tasks", "task_deletion"),
        ("show project status", "project_status"),
        ("switch to project frontend", "project_switch"),
        ("show metrics for today", "metrics"),
        ("help me with commands", "help"),
    ])
    def test_intent_detection(self, processor, input_text, expected_intent):
        """Test intent detection for various commands"""
        result = processor.process_command(input_text)
        assert result['success'] is True
        assert result['command']['intent'] == expected_intent
        assert result['command']['confidence'] >= 0.7

    # Parameter Extraction Tests
    @pytest.mark.parametrize("input_text,expected_params", [
        ("create high priority task for API integration", {
            'priority': 'high',
            'title': 'API integration'
        }),
        ("update task AUTH-001 status to completed", {
            'task_id': 'AUTH-001',
            'status': 'completed'
        }),
        ("list tasks with priority high and status active", {
            'priority': ['high'],
            'status': ['active']
        }),
    ])
    def test_parameter_extraction(self, processor, input_text, expected_params):
        """Test parameter extraction from commands"""
        result = processor.process_command(input_text)
        assert result['success'] is True
        command = result['command']
        for key, value in expected_params.items():
            assert key in command['action']['properties'] or key in command['action']['filters']

    # Error Handling Tests
    def test_empty_input_handling(self, processor):
        """Test handling of empty input"""
        result = processor.process_command("")
        assert result['success'] is False
        assert 'error' in result

    def test_unrecognized_command(self, processor):
        """Test handling of unrecognized commands"""
        result = processor.process_command("do something random unrelated")
        assert result['success'] is False
        assert result.get('command', {}).get('confidence', 1.0) < 0.5

    # Pattern Learning Tests
    @pytest.mark.asyncio
    async def test_pattern_learning(self, processor, mock_db_connection):
        """Test pattern learning from successful commands"""
        conn, cursor = mock_db_connection

        # Simulate successful command
        processor._record_pattern("create task", "task_creation", 0.9, True)

        # Verify pattern was recorded
        cursor.execute.assert_called()
        assert "INSERT INTO cometa_patterns" in str(cursor.execute.call_args)

    # Confidence Score Tests
    def test_confidence_calculation(self, processor):
        """Test confidence score calculation"""
        test_cases = [
            ("create task for testing", 0.95),  # Clear intent
            ("maybe create something", 0.6),     # Ambiguous
            ("crete tsk fr testng", 0.7),        # Typos
        ]

        for input_text, min_confidence in test_cases:
            result = processor.process_command(input_text)
            if result['success']:
                assert result['command']['confidence'] >= min_confidence
```

### 2.2 Task Executor Tests
```python
# tests/unit/test_task_executor.py

import pytest
import sqlite3
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock
from cometa_task_executor import TaskCommandExecutor

class TestTaskExecutor:
    """Test suite for Task Command Executor"""

    @pytest.fixture
    def executor(self, tmp_path):
        """Create executor with test database"""
        test_db = tmp_path / "test.db"
        # Initialize test database schema
        self._init_test_db(test_db)
        return TaskCommandExecutor(test_db)

    def _init_test_db(self, db_path):
        """Initialize test database with schema"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE task_contexts (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                description TEXT,
                priority TEXT,
                status TEXT,
                complexity_score INTEGER,
                estimated_duration_minutes INTEGER,
                created_at TEXT,
                updated_at TEXT,
                completed_at TEXT
            )
        """)
        cursor.execute("""
            CREATE TABLE projects (
                id INTEGER PRIMARY KEY,
                name TEXT,
                description TEXT,
                status TEXT,
                updated_at TEXT
            )
        """)
        conn.commit()
        conn.close()

    # Task Creation Tests
    def test_create_task_success(self, executor):
        """Test successful task creation"""
        command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {
                    'title': 'Test Task',
                    'description': 'Test Description',
                    'priority': 'high'
                }
            }
        }

        result = executor.execute_command(command)

        assert result['success'] is True
        assert 'task_id' in result['data']
        assert result['data']['task']['title'] == 'Test Task'

    def test_create_task_without_title(self, executor):
        """Test task creation without required title"""
        command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {
                    'description': 'No title task'
                }
            }
        }

        result = executor.execute_command(command)

        assert result['success'] is False
        assert 'Title is required' in result['error']

    # Task Update Tests
    def test_update_task_success(self, executor):
        """Test successful task update"""
        # First create a task
        create_command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {'title': 'Original Task'}
            }
        }
        create_result = executor.execute_command(create_command)
        task_id = create_result['data']['task_id']

        # Update the task
        update_command = {
            'action': {
                'type': 'task_management',
                'operation': 'update',
                'target': {'id': task_id},
                'properties': {
                    'status': 'in_progress',
                    'priority': 'critical'
                }
            }
        }

        result = executor.execute_command(update_command)

        assert result['success'] is True
        assert result['data']['task']['status'] == 'in_progress'
        assert result['data']['task']['priority'] == 'critical'

    # Task Completion Tests
    def test_complete_task(self, executor):
        """Test task completion"""
        # Create task
        create_command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {'title': 'Task to Complete'}
            }
        }
        create_result = executor.execute_command(create_command)

        # Complete task
        complete_command = {
            'action': {
                'type': 'task_management',
                'operation': 'complete',
                'target': {'title': 'Task to Complete'}
            }
        }

        result = executor.execute_command(complete_command)

        assert result['success'] is True
        assert result['data']['task']['status'] == 'completed'
        assert 'completed_at' in result['data']['task']

    # Transaction Tests
    @patch('sqlite3.connect')
    def test_transaction_rollback_on_error(self, mock_connect):
        """Test transaction rollback on error"""
        mock_conn = Mock()
        mock_cursor = Mock()
        mock_cursor.execute.side_effect = sqlite3.Error("Database error")
        mock_conn.cursor.return_value = mock_cursor
        mock_connect.return_value = mock_conn

        executor = TaskCommandExecutor(Path("test.db"))

        command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {'title': 'Will Fail'}
            }
        }

        result = executor.execute_command(command)

        assert result['success'] is False
        mock_conn.rollback.assert_called_once()
```

### 2.3 Progress Tracker Tests
```python
# tests/unit/test_progress_tracker.py

import pytest
from datetime import datetime, timedelta
from unittest.mock import Mock, patch
from cometa_progress_tracker import ProgressTracker

class TestProgressTracker:
    """Test suite for Progress Tracker"""

    @pytest.fixture
    def tracker(self, tmp_path):
        """Create tracker with test database"""
        test_db = tmp_path / "test.db"
        self._init_test_db(test_db)
        return ProgressTracker(test_db)

    # Metrics Calculation Tests
    def test_calculate_metrics(self, tracker):
        """Test metrics calculation"""
        # Add test data
        self._add_test_tasks(tracker.db_path)

        summary = tracker.get_progress_summary('today')

        assert summary['success'] is True
        assert 'metrics' in summary['data']
        assert 'completion_rate' in summary['data']['metrics']
        assert 'average_completion_time' in summary['data']['metrics']

    # Trend Analysis Tests
    def test_productivity_trend_analysis(self, tracker):
        """Test productivity trend calculation"""
        self._add_historical_tasks(tracker.db_path)

        trends = tracker.analyze_productivity_trends('week')

        assert trends['success'] is True
        assert 'trend_direction' in trends['data']
        assert trends['data']['trend_direction'] in ['increasing', 'decreasing', 'stable']

    # Natural Language Generation Tests
    @pytest.mark.parametrize("metrics,expected_phrases", [
        ({'completion_rate': 0.9}, ["excellent progress", "90%"]),
        ({'completion_rate': 0.5}, ["moderate progress", "50%"]),
        ({'completion_rate': 0.2}, ["needs attention", "20%"]),
    ])
    def test_natural_language_insights(self, tracker, metrics, expected_phrases):
        """Test natural language insight generation"""
        insights = tracker._generate_insights(None, metrics, 'today')

        nl_summary = insights['natural_language_summary'].lower()
        for phrase in expected_phrases:
            assert phrase in nl_summary
```

### 2.4 Batch Manager Tests
```python
# tests/unit/test_batch_manager.py

import pytest
import asyncio
from unittest.mock import Mock, patch, call
from concurrent.futures import ThreadPoolExecutor
from cometa_batch_manager import BatchOperationsManager

class TestBatchManager:
    """Test suite for Batch Operations Manager"""

    @pytest.fixture
    def manager(self, tmp_path):
        """Create manager with test database"""
        test_db = tmp_path / "test.db"
        return BatchOperationsManager(test_db)

    # Sequential Execution Tests
    def test_sequential_execution(self, manager):
        """Test sequential batch execution"""
        batch_request = {
            'commands': [
                {'intent': 'create', 'action': {'type': 'task_management'}},
                {'intent': 'update', 'action': {'type': 'task_management'}},
                {'intent': 'complete', 'action': {'type': 'task_management'}}
            ],
            'execution_mode': 'sequential'
        }

        with patch.object(manager, '_execute_single_command') as mock_execute:
            mock_execute.return_value = {'success': True}

            result = manager.execute_batch(batch_request)

            assert result['success'] is True
            assert result['execution_mode'] == 'sequential'
            assert mock_execute.call_count == 3

    # Parallel Execution Tests
    def test_parallel_execution(self, manager):
        """Test parallel batch execution"""
        batch_request = {
            'commands': [
                {'intent': 'task1'},
                {'intent': 'task2'},
                {'intent': 'task3'}
            ],
            'execution_mode': 'parallel'
        }

        with patch.object(manager, '_execute_single_command') as mock_execute:
            mock_execute.return_value = {'success': True}

            result = manager.execute_batch(batch_request)

            assert result['success'] is True
            assert result['execution_mode'] == 'parallel'
            assert result['completed'] == 3

    # Conditional Execution Tests
    def test_conditional_execution(self, manager):
        """Test conditional batch execution"""
        batch_request = {
            'commands': [
                {'intent': 'check'},
                {'intent': 'proceed'}
            ],
            'execution_mode': 'conditional',
            'conditions': [
                {'command_index': 1, 'if': 'previous_success'}
            ]
        }

        with patch.object(manager, '_execute_single_command') as mock_execute:
            # First command fails
            mock_execute.side_effect = [
                {'success': False},
                {'success': True}
            ]

            result = manager.execute_batch(batch_request)

            # Second command should be skipped
            assert result['results'][1]['result']['skipped'] is True

    # Rollback Tests
    def test_rollback_on_error(self, manager):
        """Test rollback mechanism on error"""
        batch_request = {
            'commands': [
                {'intent': 'task1'},
                {'intent': 'task2'},
                {'intent': 'task3'}
            ],
            'execution_mode': 'sequential',
            'rollback_on_error': True
        }

        with patch.object(manager, '_execute_single_command') as mock_execute:
            # Second command fails
            mock_execute.side_effect = [
                {'success': True},
                {'success': False, 'error': 'Test error'},
                {'success': True}
            ]

            result = manager.execute_batch(batch_request)

            # Third command should not be executed
            assert mock_execute.call_count == 2
            assert result['failed'] == 1
```

---

## 3. INTEGRATION TESTING

### 3.1 Database Integration Tests
```python
# tests/integration/test_database_integration.py

import pytest
import sqlite3
from pathlib import Path

@pytest.fixture(scope='function')
def test_database(tmp_path):
    """Create a test database with full schema"""
    db_path = tmp_path / "integration_test.db"

    # Initialize complete schema
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Create all tables
    with open('schema/devflow_unified_schema.sql', 'r') as f:
        cursor.executescript(f.read())

    conn.commit()
    conn.close()

    yield db_path

    # Cleanup
    if db_path.exists():
        db_path.unlink()

class TestDatabaseIntegration:
    """Integration tests for database operations"""

    def test_end_to_end_task_lifecycle(self, test_database):
        """Test complete task lifecycle through all components"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor
        from cometa_task_executor import TaskCommandExecutor
        from cometa_progress_tracker import ProgressTracker

        # Initialize components
        processor = NaturalLanguageCommandProcessor(test_database)
        executor = TaskCommandExecutor(test_database)
        tracker = ProgressTracker(test_database)

        # Process natural language command
        nl_command = "create high priority task for implementing user authentication with 2FA"
        process_result = processor.process_command(nl_command)
        assert process_result['success'] is True

        # Execute command
        exec_result = executor.execute_command(process_result['command'])
        assert exec_result['success'] is True
        task_id = exec_result['data']['task_id']

        # Track progress
        progress = tracker.get_progress_summary('today')
        assert progress['success'] is True
        assert progress['data']['metrics']['total_tasks'] > 0

        # Update task
        update_command = "complete task authentication"
        update_result = processor.process_command(update_command)
        exec_update = executor.execute_command(update_result['command'])
        assert exec_update['success'] is True

        # Verify completion
        final_progress = tracker.get_progress_summary('today')
        assert final_progress['data']['metrics']['completed_tasks'] > 0
```

### 3.2 Hook System Integration Tests
```python
# tests/integration/test_hook_integration.py

import pytest
import json
from pathlib import Path

class TestHookIntegration:
    """Test hook system integration"""

    def test_nlp_hook_trigger_detection(self):
        """Test NLP hook trigger detection"""
        from cometa_nlp_hook import CometaBrainHook

        hook = CometaBrainHook()

        test_prompts = [
            "brain: create task for testing",
            "task: list all active tasks",
            "cometa: show project status",
            "Regular text without trigger"
        ]

        for prompt in test_prompts[:-1]:
            command = hook._extract_nlp_command(prompt)
            assert command is not None

        # Last prompt should not trigger
        assert hook._extract_nlp_command(test_prompts[-1]) is None

    def test_hook_end_to_end_processing(self, test_database):
        """Test complete hook processing pipeline"""
        from cometa_nlp_hook import CometaBrainHook

        hook = CometaBrainHook()
        hook.db_path = test_database

        # Simulate hook invocation
        result = hook.process_user_prompt("brain: create task for API documentation")

        assert result is not None
        assert 'formatted_output' in result
        assert '‚úÖ' in result['formatted_output'] or '‚ùå' in result['formatted_output']
```

---

## 4. END-TO-END TESTING

### 4.1 Playwright E2E Tests
```python
# tests/e2e/test_e2e_workflows.py

import pytest
from playwright.sync_api import Page, expect
import asyncio
from playwright.async_api import async_playwright

class TestE2EWorkflows:
    """End-to-end test workflows"""

    @pytest.fixture(scope='function')
    async def browser_context(self):
        """Setup browser context for testing"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()
            yield page
            await browser.close()

    async def test_complete_task_workflow(self, browser_context):
        """Test complete task creation and management workflow"""
        page = browser_context

        # Navigate to application
        await page.goto('http://localhost:3000')

        # Enter natural language command
        command_input = page.locator('#nl-command-input')
        await command_input.fill('create high priority task for user authentication')
        await command_input.press('Enter')

        # Verify task creation feedback
        await expect(page.locator('.success-message')).to_be_visible()
        await expect(page.locator('.task-created')).to_contain_text('authentication')

        # List tasks
        await command_input.fill('list all active tasks')
        await command_input.press('Enter')

        # Verify task appears in list
        task_list = page.locator('.task-list')
        await expect(task_list).to_contain_text('user authentication')

        # Complete task
        await command_input.fill('complete task authentication')
        await command_input.press('Enter')

        # Verify completion
        await expect(page.locator('.task-completed')).to_be_visible()

    async def test_batch_operations_workflow(self, browser_context):
        """Test batch operations workflow"""
        page = browser_context

        await page.goto('http://localhost:3000/batch')

        # Create batch workflow
        batch_commands = [
            "create task for frontend development",
            "create task for backend API",
            "create task for database schema"
        ]

        for cmd in batch_commands:
            await page.locator('.batch-command-input').fill(cmd)
            await page.click('.add-command')

        # Execute batch
        await page.click('#execute-batch')

        # Verify all tasks created
        results = page.locator('.batch-results')
        await expect(results).to_contain_text('3 commands completed')
```

### 4.2 API E2E Tests
```python
# tests/e2e/test_api_e2e.py

import pytest
import httpx
import asyncio

class TestAPIE2E:
    """API end-to-end tests"""

    @pytest.fixture
    async def client(self):
        """Create async HTTP client"""
        async with httpx.AsyncClient(base_url="http://localhost:8000") as client:
            yield client

    async def test_nlp_api_workflow(self, client):
        """Test NLP API workflow"""
        # Create task via NLP
        response = await client.post("/api/nlp/process", json={
            "command": "create high priority task for API testing"
        })

        assert response.status_code == 200
        data = response.json()
        assert data['success'] is True
        task_id = data['data']['task_id']

        # Get task details
        task_response = await client.get(f"/api/tasks/{task_id}")
        assert task_response.status_code == 200
        task = task_response.json()
        assert task['priority'] == 'high'

        # Update via NLP
        update_response = await client.post("/api/nlp/process", json={
            "command": f"update task {task_id} status to in_progress"
        })
        assert update_response.status_code == 200

        # Complete via NLP
        complete_response = await client.post("/api/nlp/process", json={
            "command": f"complete task {task_id}"
        })
        assert complete_response.status_code == 200
```

---

## 5. PERFORMANCE TESTING

### 5.1 Load Testing
```python
# tests/performance/test_load.py

import pytest
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
import statistics

class TestPerformance:
    """Performance and load tests"""

    def test_concurrent_command_processing(self):
        """Test concurrent command processing performance"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        commands = [
            f"create task for testing item {i}"
            for i in range(100)
        ]

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(processor.process_command, commands))

        end_time = time.time()

        # Performance assertions
        total_time = end_time - start_time
        avg_time = total_time / len(commands)

        assert avg_time < 0.1  # Less than 100ms per command
        assert all(r['success'] for r in results)

    @pytest.mark.benchmark(group="nlp")
    def test_nlp_processing_benchmark(self, benchmark):
        """Benchmark NLP processing"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        result = benchmark(
            processor.process_command,
            "create high priority task for performance testing"
        )

        assert result['success'] is True

    def test_database_transaction_performance(self):
        """Test database transaction performance"""
        from cometa_task_executor import TaskCommandExecutor

        executor = TaskCommandExecutor(Path("test.db"))

        # Measure bulk insert performance
        commands = [
            {
                'action': {
                    'type': 'task_management',
                    'operation': 'create',
                    'properties': {'title': f'Task {i}'}
                }
            }
            for i in range(1000)
        ]

        start_time = time.time()

        for cmd in commands:
            executor.execute_command(cmd)

        end_time = time.time()

        # Should handle 1000 inserts in under 10 seconds
        assert (end_time - start_time) < 10
```

### 5.2 Memory Profiling
```python
# tests/performance/test_memory.py

import pytest
import tracemalloc
import gc

class TestMemoryUsage:
    """Memory usage tests"""

    def test_memory_leak_detection(self):
        """Detect memory leaks in components"""
        from cometa_batch_manager import BatchOperationsManager

        tracemalloc.start()

        # Create and destroy many instances
        for _ in range(1000):
            manager = BatchOperationsManager(Path("test.db"))
            manager.execute_batch({
                'commands': [{'intent': 'test'}],
                'execution_mode': 'sequential'
            })
            del manager
            gc.collect()

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        # Memory usage should be reasonable (< 50MB)
        assert peak / 1024 / 1024 < 50
```

---

## 6. SECURITY TESTING

### 6.1 SQL Injection Tests
```python
# tests/security/test_sql_injection.py

import pytest

class TestSQLInjection:
    """SQL injection prevention tests"""

    def test_sql_injection_prevention(self):
        """Test SQL injection prevention"""
        from cometa_task_executor import TaskCommandExecutor

        executor = TaskCommandExecutor(Path("test.db"))

        # Attempt SQL injection
        malicious_commands = [
            "'; DROP TABLE task_contexts; --",
            "1' OR '1'='1",
            "'; UPDATE task_contexts SET status='hacked'; --"
        ]

        for malicious_input in malicious_commands:
            command = {
                'action': {
                    'type': 'task_management',
                    'operation': 'create',
                    'properties': {'title': malicious_input}
                }
            }

            result = executor.execute_command(command)

            # Should handle safely without executing injection
            # Verify tables still exist
            conn = sqlite3.connect("test.db")
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            conn.close()

            assert any('task_contexts' in t for t in tables)
```

### 6.2 Input Validation Tests
```python
# tests/security/test_input_validation.py

class TestInputValidation:
    """Input validation tests"""

    def test_command_size_limits(self):
        """Test command size limits"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Test extremely long input
        long_input = "create task " + "x" * 10000
        result = processor.process_command(long_input)

        # Should handle gracefully
        assert 'error' in result or len(result['command']['action']['properties']['title']) < 1000
```

---

## 7. CI/CD PIPELINE

### 7.1 GitHub Actions Workflow
```yaml
# .github/workflows/test-and-deploy.yml

name: Test and Deploy Cometa Brain

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - run: |
          pip install ruff black mypy
          ruff check .
          black --check .
          mypy --strict .

  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt

      - name: Run unit tests
        run: |
          pytest tests/unit -v --cov=cometa --cov-report=xml

      - name: Run integration tests
        run: |
          pytest tests/integration -v

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true

  e2e-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
      - uses: microsoft/playwright-github-action@v1

      - name: Install dependencies
        run: |
          pip install playwright pytest-playwright
          playwright install

      - name: Run E2E tests
        run: |
          pytest tests/e2e -v --browser=chromium

  performance-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4

      - name: Run performance tests
        run: |
          pytest tests/performance -v --benchmark-only

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true

  deploy:
    needs: [lint, test, e2e-tests]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Deploy to production
        run: |
          # Deployment script
          echo "Deploying to production..."
```

### 7.2 Pre-commit Hooks
```yaml
# .pre-commit-config.yaml

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-ast
      - id: check-merge-conflict

  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black
        language_version: python3.10

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.1.8
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: local
    hooks:
      - id: pytest-unit
        name: Run unit tests
        entry: pytest tests/unit -x
        language: system
        pass_filenames: false
        always_run: true
```

---

## 8. DEBUG TOOLS & MONITORING

### 8.1 Debug Utilities
```python
# tools/debug_utils.py

import logging
import functools
import time
from contextlib import contextmanager

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cometa_debug.log'),
        logging.StreamHandler()
    ]
)

def debug_timer(func):
    """Decorator to time function execution"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        logging.debug(f"{func.__name__} took {end-start:.4f} seconds")
        return result
    return wrapper

@contextmanager
def debug_context(operation_name):
    """Context manager for debugging operations"""
    logging.debug(f"Starting: {operation_name}")
    start = time.time()
    try:
        yield
    except Exception as e:
        logging.error(f"Error in {operation_name}: {str(e)}", exc_info=True)
        raise
    finally:
        end = time.time()
        logging.debug(f"Completed: {operation_name} in {end-start:.4f}s")

class CommandDebugger:
    """Debug helper for command processing"""

    @staticmethod
    def log_command(command, phase="received"):
        """Log command at different phases"""
        logging.debug(f"Command {phase}: {json.dumps(command, indent=2)}")

    @staticmethod
    def log_database_query(query, params=None):
        """Log database queries"""
        logging.debug(f"SQL: {query}")
        if params:
            logging.debug(f"Params: {params}")
```

### 8.2 Monitoring Setup
```python
# monitoring/metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time

# Metrics
command_counter = Counter(
    'cometa_commands_total',
    'Total number of commands processed',
    ['intent', 'status']
)

command_duration = Histogram(
    'cometa_command_duration_seconds',
    'Command processing duration',
    ['operation']
)

active_tasks = Gauge(
    'cometa_active_tasks',
    'Number of active tasks'
)

database_connections = Gauge(
    'cometa_db_connections',
    'Number of database connections'
)

class MetricsCollector:
    """Collect and export metrics"""

    @staticmethod
    def record_command(intent, success):
        """Record command execution"""
        status = 'success' if success else 'failure'
        command_counter.labels(intent=intent, status=status).inc()

    @staticmethod
    def time_operation(operation):
        """Decorator to time operations"""
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                with command_duration.labels(operation=operation).time():
                    return func(*args, **kwargs)
            return wrapper
        return decorator
```

---

## 9. DEPLOYMENT CHECKLIST

### 9.1 Pre-Deployment Validation
```bash
#!/bin/bash
# scripts/pre-deploy-check.sh

echo "üîç Running Pre-Deployment Checks..."

# 1. Run all tests
echo "Running tests..."
pytest tests/ -v --cov=cometa --cov-fail-under=95

# 2. Check code quality
echo "Checking code quality..."
ruff check .
black --check .
mypy --strict .

# 3. Security scan
echo "Running security scan..."
bandit -r cometa/ -f json -o security-report.json

# 4. Check dependencies
echo "Checking dependencies..."
safety check --json

# 5. Database migration check
echo "Checking database migrations..."
alembic check

# 6. Performance benchmarks
echo "Running performance benchmarks..."
pytest tests/performance --benchmark-compare

# 7. Documentation check
echo "Checking documentation..."
pydoc-markdown --check

echo "‚úÖ Pre-deployment checks complete!"
```

### 9.2 Production Configuration
```python
# config/production.py

import os
from pathlib import Path

class ProductionConfig:
    """Production configuration"""

    # Database
    DATABASE_PATH = Path(os.environ.get('COMETA_DB_PATH', '/var/lib/cometa/devflow_unified.sqlite'))
    DATABASE_POOL_SIZE = 20
    DATABASE_TIMEOUT = 30

    # Performance
    MAX_WORKERS = 10
    COMMAND_TIMEOUT = 60
    CACHE_SIZE = 1000

    # Security
    MAX_COMMAND_LENGTH = 5000
    RATE_LIMIT = 100  # commands per minute

    # Monitoring
    ENABLE_METRICS = True
    METRICS_PORT = 9090

    # Logging
    LOG_LEVEL = 'INFO'
    LOG_FILE = '/var/log/cometa/cometa.log'

    # Feature flags
    ENABLE_PATTERN_LEARNING = True
    ENABLE_BATCH_OPERATIONS = True
    ENABLE_ASYNC_PROCESSING = True
```

### 9.3 Deployment Script
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "üöÄ Deploying Cometa Brain to Production..."

# 1. Backup existing deployment
echo "Creating backup..."
tar -czf backup-$(date +%Y%m%d-%H%M%S).tar.gz /opt/cometa/

# 2. Update code
echo "Updating code..."
git pull origin main

# 3. Install dependencies
echo "Installing dependencies..."
pip install -r requirements.txt --upgrade

# 4. Run database migrations
echo "Running database migrations..."
alembic upgrade head

# 5. Run tests
echo "Running production tests..."
pytest tests/smoke/ -v

# 6. Update configuration
echo "Updating configuration..."
cp config/production.py /opt/cometa/config.py

# 7. Restart services
echo "Restarting services..."
systemctl restart cometa-brain
systemctl restart cometa-worker

# 8. Health check
echo "Running health checks..."
curl -f http://localhost:8000/health || exit 1

echo "‚úÖ Deployment complete!"
```

---

## 10. POST-DEPLOYMENT VALIDATION

### 10.1 Smoke Tests
```python
# tests/smoke/test_smoke.py

import pytest
import httpx
import time

def test_service_health():
    """Test service is healthy"""
    response = httpx.get("http://localhost:8000/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_basic_command_processing():
    """Test basic command processing works"""
    response = httpx.post("http://localhost:8000/api/nlp/process", json={
        "command": "help"
    })
    assert response.status_code == 200
    assert response.json()["success"] is True

def test_database_connectivity():
    """Test database is accessible"""
    response = httpx.get("http://localhost:8000/api/metrics")
    assert response.status_code == 200
    assert "total_tasks" in response.json()
```

### 10.2 Monitoring Dashboard
```yaml
# monitoring/grafana-dashboard.json
{
  "dashboard": {
    "title": "Cometa Brain Monitoring",
    "panels": [
      {
        "title": "Command Processing Rate",
        "targets": [
          {"expr": "rate(cometa_commands_total[5m])"}
        ]
      },
      {
        "title": "Success Rate",
        "targets": [
          {"expr": "rate(cometa_commands_total{status='success'}[5m]) / rate(cometa_commands_total[5m])"}
        ]
      },
      {
        "title": "Response Time",
        "targets": [
          {"expr": "histogram_quantile(0.95, cometa_command_duration_seconds)"}
        ]
      },
      {
        "title": "Active Tasks",
        "targets": [
          {"expr": "cometa_active_tasks"}
        ]
      }
    ]
  }
}
```

---

## 11. CONFORMIT√Ä ARCHITETTURA ORIGINALE - VALIDAZIONE SPECIFICA

### 11.1 Test 4-Layer Intelligence Architecture
```python
# tests/conformity/test_4_layer_architecture.py

import pytest
from pathlib import Path
from unittest.mock import Mock, patch
import time

class Test4LayerArchitecture:
    """Tests for 4-Layer Intelligence Architecture conformity"""

    def test_layer1_hook_intelligence_engine(self):
        """Test Layer 1: Hook Intelligence Engine"""
        from cometa_nlp_hook import CometaBrainHook

        hook = CometaBrainHook()

        # Test trigger detection (Layer 1 responsibility)
        test_cases = [
            ("brain: create task", True),
            ("task: show status", True),
            ("cometa: help", True),
            ("regular text", False)
        ]

        for prompt, should_trigger in test_cases:
            result = hook._should_trigger(prompt)
            assert result == should_trigger, f"Layer 1 failed for: {prompt}"

        # Test confidence threshold (>70% per spec)
        command_result = hook.process_user_prompt("brain: create high priority task")
        if command_result:
            assert command_result.get('confidence', 0) >= 0.7

    def test_layer2_context_memory_authority(self):
        """Test Layer 2: Context & Memory Authority"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Test context preservation across commands
        commands = [
            "create task for authentication",
            "set priority to high",  # Should reference previous task
            "add deadline for next week"
        ]

        context = {}
        for cmd in commands:
            result = processor.process_command(cmd, context=context)
            assert result['success'], f"Context authority failed for: {cmd}"

            # Update context for next command
            if result.get('context_updates'):
                context.update(result['context_updates'])

        # Verify context continuity (<5 seconds per spec)
        start_time = time.time()
        processor.restore_session_context("test_session_id")
        restore_time = time.time() - start_time
        assert restore_time < 5.0, f"Context restore took {restore_time}s, exceeds 5s limit"

    def test_layer3_task_management_override(self):
        """Test Layer 3: Task Management Override (Authority Centralization)"""
        from cometa_task_executor import TaskCommandExecutor

        executor = TaskCommandExecutor(Path("test.db"))

        # Test task creation override (should bypass Claude Code)
        override_command = {
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {'title': 'Override Test Task'},
                'override_claude_code': True
            }
        }

        result = executor.execute_command(override_command)
        assert result['success'], "Task override failed"
        assert result.get('override_applied'), "Override not applied"

        # Test authority validation (100% task override per spec)
        authority_metrics = executor.get_authority_metrics()
        assert authority_metrics['override_rate'] == 1.0, "Not achieving 100% task override"

    def test_layer4_learning_evolution(self):
        """Test Layer 4: Learning & Evolution"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor
        from cometa_batch_manager import BatchOperationsManager

        processor = NaturalLanguageCommandProcessor(Path("test.db"))
        batch_manager = BatchOperationsManager(Path("test.db"))

        # Test pattern learning
        successful_patterns = [
            "create task for testing feature X",
            "create task for implementing Y",
            "create task for debugging Z"
        ]

        # Process commands to build patterns
        for pattern in successful_patterns:
            result = processor.process_command(pattern)
            processor.record_learning_success(pattern, result)

        # Test pattern application on new similar command
        test_command = "create task for developing new feature"
        result = processor.process_command(test_command)

        # Should use learned patterns for higher confidence
        assert result.get('pattern_applied'), "Pattern learning not applied"
        assert result['command']['confidence'] > 0.8, "Learning not improving confidence"

        # Test cross-session intelligence
        new_processor = NaturalLanguageCommandProcessor(Path("test.db"))
        cross_result = new_processor.process_command("create task for another feature")
        assert cross_result.get('pattern_applied'), "Cross-session learning failed"
```

### 11.2 Test Authority Centralization Compliance
```python
# tests/conformity/test_authority_centralization.py

import pytest
from pathlib import Path
from unittest.mock import Mock, patch

class TestAuthorityCentralization:
    """Test Authority Centralization pattern compliance"""

    def test_claude_code_task_override(self):
        """Test that Cometa Brain overrides Claude Code task system"""
        from cometa_task_executor import TaskCommandExecutor

        executor = TaskCommandExecutor(Path("test.db"))

        # Simulate Claude Code task creation attempt
        claude_task = {
            'source': 'claude_code',
            'action': {
                'type': 'task_management',
                'operation': 'create',
                'properties': {'title': 'Claude Generated Task'}
            }
        }

        # Should be intercepted and processed by Cometa
        result = executor.execute_command(claude_task)

        assert result['success']
        assert result.get('intercepted_by_cometa'), "Task not intercepted by Cometa"
        assert result.get('override_source') == 'cometa_brain'

        # Verify in database that task is marked as Cometa-managed
        conn = sqlite3.connect("test.db")
        cursor = conn.cursor()
        cursor.execute("SELECT source FROM task_contexts WHERE id = ?",
                      (result['data']['task_id'],))
        source = cursor.fetchone()[0]
        assert source == 'cometa_brain'

    def test_authority_metrics_compliance(self):
        """Test authority metrics meet specification targets"""
        from cometa_progress_tracker import ProgressTracker

        tracker = ProgressTracker(Path("test.db"))

        # Generate test data with required override patterns
        self._generate_authority_test_data()

        metrics = tracker.get_authority_metrics()

        # Test specification compliance
        assert metrics['task_override_rate'] >= 1.0, f"Task override rate {metrics['task_override_rate']} < 100%"
        assert metrics['context_control_rate'] >= 0.85, f"Context control rate {metrics['context_control_rate']} < 85%"
        assert metrics['decision_authority_rate'] >= 0.90, f"Decision authority {metrics['decision_authority_rate']} < 90%"

    def test_hook_authority_enforcement(self):
        """Test hook system enforces Cometa authority"""
        from cometa_nlp_hook import CometaBrainHook

        hook = CometaBrainHook()

        # Test hook prevents direct Claude Code task creation
        claude_prompt = "Let me create a task manually"
        hook_result = hook.pre_tool_use_hook({
            'tool': 'Write',
            'parameters': {'file_path': 'tasks/new_task.md'}
        })

        # Should block or redirect through Cometa
        if hook_result:
            assert 'redirected_to_cometa' in hook_result
            assert hook_result['allow_direct_creation'] is False
```

### 11.3 Test KPI Business Requirements
```python
# tests/conformity/test_kpi_compliance.py

import pytest
import time
from pathlib import Path
from datetime import datetime, timedelta

class TestKPICompliance:
    """Test KPI business requirements compliance"""

    def test_task_auto_creation_rate(self):
        """Test >80% task auto-creation rate (Target: 85%)"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor
        from cometa_task_executor import TaskCommandExecutor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))
        executor = TaskCommandExecutor(Path("test.db"))

        # Test batch of natural language commands
        test_commands = [
            "need to implement user authentication",
            "bug in the payment processor needs fixing",
            "add search functionality to the app",
            "refactor the database layer",
            "write unit tests for API endpoints",
            "update documentation for new features",
            "optimize performance of dashboard",
            "integrate with third-party service",
            "handle edge cases in error handling",
            "deploy to staging environment"
        ]

        auto_created = 0
        total_processed = 0

        for cmd in test_commands:
            result = processor.process_command(cmd)
            if result['success']:
                exec_result = executor.execute_command(result['command'])
                total_processed += 1
                if exec_result.get('auto_created'):
                    auto_created += 1

        auto_creation_rate = auto_created / total_processed if total_processed > 0 else 0
        assert auto_creation_rate >= 0.80, f"Auto-creation rate {auto_creation_rate:.2%} < 80%"
        print(f"‚úÖ Auto-creation rate: {auto_creation_rate:.2%} (Target: 85%)")

    def test_context_relevance_score(self):
        """Test >85% context relevance score (Target: 90%)"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Setup context with previous tasks
        context = {
            'current_project': 'e-commerce-app',
            'active_tasks': ['auth-impl', 'payment-bug'],
            'recent_topics': ['authentication', 'payments', 'security']
        }

        # Test context-aware commands
        contextual_commands = [
            "update the auth task priority",  # Should reference auth-impl
            "fix payment processing error",   # Should reference payment-bug
            "add security headers",           # Should relate to security topic
            "test the login flow",           # Should relate to authentication
        ]

        relevance_scores = []
        for cmd in contextual_commands:
            result = processor.process_command(cmd, context=context)
            if result['success']:
                relevance_scores.append(result.get('context_relevance_score', 0))

        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        assert avg_relevance >= 0.85, f"Context relevance {avg_relevance:.2%} < 85%"
        print(f"‚úÖ Context relevance: {avg_relevance:.2%} (Target: 90%)")

    def test_session_continuity_performance(self):
        """Test <5 seconds session continuity"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Create session with context
        session_id = "test_session_001"
        processor.create_session_context(session_id, {
            'tasks': [{'id': 1, 'title': 'Test Task'}],
            'context': {'project': 'test'},
            'memory': ['previous interaction 1', 'previous interaction 2']
        })

        # Test session restoration performance
        start_time = time.time()
        restored_context = processor.restore_session_context(session_id)
        restore_time = time.time() - start_time

        assert restore_time < 5.0, f"Session continuity {restore_time:.2f}s > 5s limit"
        assert restored_context is not None, "Session context not restored"
        print(f"‚úÖ Session continuity: {restore_time:.2f}s (Target: <5s)")

    def test_hook_performance_target(self):
        """Test <500ms hook execution (Target: <300ms)"""
        from cometa_nlp_hook import CometaBrainHook

        hook = CometaBrainHook()

        test_prompts = [
            "brain: create task for testing",
            "task: show current status",
            "cometa: help with commands",
            "brain: list active tasks"
        ]

        execution_times = []
        for prompt in test_prompts:
            start_time = time.time()
            hook.process_user_prompt(prompt)
            execution_time = (time.time() - start_time) * 1000  # Convert to ms
            execution_times.append(execution_time)

        avg_execution_time = sum(execution_times) / len(execution_times)
        max_execution_time = max(execution_times)

        assert avg_execution_time < 500, f"Average hook execution {avg_execution_time:.1f}ms > 500ms"
        assert max_execution_time < 500, f"Max hook execution {max_execution_time:.1f}ms > 500ms"
        print(f"‚úÖ Hook performance: {avg_execution_time:.1f}ms avg (Target: <300ms)")
```

### 11.4 Test Natural Language Interface Compliance
```python
# tests/conformity/test_nl_interface_compliance.py

import pytest
from pathlib import Path

class TestNLInterfaceCompliance:
    """Test Natural Language Interface compliance with original design"""

    def test_command_pattern_recognition(self):
        """Test recognition of original command patterns"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Original spec command patterns
        original_patterns = {
            'task_creation': [
                "create task for implementing feature X",
                "need to work on bug fix Y",
                "add task for testing component Z"
            ],
            'task_management': [
                "update task priority to high",
                "mark task as completed",
                "assign task to developer"
            ],
            'project_queries': [
                "show project status",
                "list active tasks",
                "get progress summary"
            ],
            'system_commands': [
                "help with task commands",
                "show available options",
                "explain how to use the system"
            ]
        }

        for intent, patterns in original_patterns.items():
            for pattern in patterns:
                result = processor.process_command(pattern)
                assert result['success'], f"Pattern recognition failed: {pattern}"
                detected_intent = result['command']['intent']
                assert intent in detected_intent or detected_intent in intent, \
                    f"Wrong intent detected for '{pattern}': got {detected_intent}, expected {intent}"

    def test_natural_language_flexibility(self):
        """Test natural language flexibility as per original spec"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Same intent, different phrasings
        flexible_commands = [
            ["create task for auth", "make a task about authentication", "need auth task"],
            ["show me the tasks", "list all tasks", "what are my tasks"],
            ["mark it done", "complete the task", "set status to finished"],
        ]

        for command_group in flexible_commands:
            intents = []
            for cmd in command_group:
                result = processor.process_command(cmd)
                if result['success']:
                    intents.append(result['command']['intent'])

            # All commands in group should have same base intent
            base_intents = [intent.split('_')[0] for intent in intents]
            assert len(set(base_intents)) <= 2, f"Intent inconsistency: {intents}"

    def test_contextual_understanding(self):
        """Test contextual understanding capability"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Build context through conversation
        conversation = [
            ("create task for user authentication", None),
            ("set its priority to high", "previous_task_reference"),
            ("add another task for testing it", "task_relationship"),
            ("how is the auth project going?", "project_context")
        ]

        context = {}
        for cmd, expected_context_type in conversation:
            result = processor.process_command(cmd, context=context)
            assert result['success'], f"Context processing failed: {cmd}"

            if expected_context_type:
                assert result.get('context_used'), f"Context not used for: {cmd}"
                assert expected_context_type in str(result.get('context_type', ''))

            # Update context
            if result.get('context_updates'):
                context.update(result['context_updates'])
```

### 11.5 Test Cross-Session Intelligence
```python
# tests/conformity/test_cross_session_intelligence.py

import pytest
from pathlib import Path
from datetime import datetime, timedelta

class TestCrossSessionIntelligence:
    """Test cross-session intelligence and learning"""

    def test_pattern_persistence(self):
        """Test that learned patterns persist across sessions"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        # Session 1: Learn patterns
        processor1 = NaturalLanguageCommandProcessor(Path("test.db"))

        learning_commands = [
            "create task for implementing API endpoint",
            "create task for writing unit tests",
            "create task for updating documentation"
        ]

        for cmd in learning_commands:
            result = processor1.process_command(cmd)
            processor1.record_pattern_success(cmd, result)

        # Session 2: Different processor instance
        processor2 = NaturalLanguageCommandProcessor(Path("test.db"))

        # Should recognize similar pattern
        test_cmd = "create task for implementing new feature"
        result = processor2.process_command(test_cmd)

        assert result['success']
        assert result.get('pattern_matched'), "Pattern not persisted across sessions"
        assert result['command']['confidence'] > 0.8, "Pattern learning not improving confidence"

    def test_memory_stream_intelligence(self):
        """Test memory stream intelligence across sessions"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor
        from cometa_progress_tracker import ProgressTracker

        processor = NaturalLanguageCommandProcessor(Path("test.db"))
        tracker = ProgressTracker(Path("test.db"))

        # Create memory events
        memory_events = [
            {"type": "task_created", "data": {"task": "auth implementation", "context": "security"}},
            {"type": "pattern_learned", "data": {"pattern": "auth-related tasks", "confidence": 0.9}},
            {"type": "user_preference", "data": {"prefers": "high priority for security tasks"}}
        ]

        for event in memory_events:
            processor.add_memory_event(event)

        # New session should use memory intelligence
        new_processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Command that should trigger memory-based intelligence
        result = new_processor.process_command("create task for password encryption")

        assert result['success']
        assert result.get('memory_influenced'), "Memory not influencing processing"

        # Should auto-assign high priority based on security preference
        if result['command']['action'].get('properties', {}).get('priority'):
            assert result['command']['action']['properties']['priority'] == 'high'

    def test_learning_evolution_metrics(self):
        """Test learning evolution metrics"""
        from cometa_progress_tracker import ProgressTracker

        tracker = ProgressTracker(Path("test.db"))

        # Generate learning data over time
        self._generate_learning_evolution_data()

        evolution_metrics = tracker.get_learning_metrics()

        # Test learning improvement over time
        assert evolution_metrics['pattern_recognition_improvement'] > 0, "No pattern recognition improvement"
        assert evolution_metrics['confidence_trend'] == 'increasing', "Confidence not increasing over time"
        assert evolution_metrics['success_rate_trend'] == 'increasing', "Success rate not improving"

        # Test learning velocity (how fast system improves)
        assert evolution_metrics['learning_velocity'] >= 0.05, "Learning velocity too slow"
```

### 11.6 Test Pattern Learning Validation
```python
# tests/conformity/test_pattern_learning.py

import pytest
from pathlib import Path

class TestPatternLearning:
    """Test pattern learning and adaptation capabilities"""

    def test_pattern_extraction_accuracy(self):
        """Test accuracy of pattern extraction from successful commands"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Training data with clear patterns
        training_data = [
            ("create high priority task for API security", "task_creation", "security"),
            ("create urgent task for database optimization", "task_creation", "performance"),
            ("create critical task for bug fixing", "task_creation", "maintenance"),
            ("list all high priority tasks", "task_listing", "priority_filter"),
            ("show urgent tasks status", "task_listing", "status_query")
        ]

        # Train pattern recognition
        for cmd, expected_intent, expected_category in training_data:
            result = processor.process_command(cmd)
            processor.record_pattern(cmd, expected_intent, expected_category, True)

        # Test pattern recognition on new data
        test_data = [
            ("create high priority task for payment processing", "task_creation", "security"),
            ("list all urgent tasks for today", "task_listing", "priority_filter")
        ]

        for cmd, expected_intent, expected_category in test_data:
            result = processor.process_command(cmd)

            assert result['success']
            assert result.get('pattern_applied'), f"No pattern applied for: {cmd}"
            assert result['command']['intent'] == expected_intent
            assert result.get('pattern_category') == expected_category

    def test_adaptive_confidence_scoring(self):
        """Test adaptive confidence scoring based on learned patterns"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))

        # Commands that should initially have lower confidence
        initial_commands = [
            "make something for the thing",  # Vague
            "handle the issue with stuff",   # Ambiguous
        ]

        for cmd in initial_commands:
            result = processor.process_command(cmd)
            initial_confidence = result.get('command', {}).get('confidence', 0)
            assert initial_confidence < 0.6, "Initial confidence too high for ambiguous command"

        # Simulate learning from successful clarifications
        clarified_patterns = [
            ("make task for the authentication feature", "task_creation", True),
            ("handle the database connection issue", "task_creation", True),
        ]

        for cmd, intent, success in clarified_patterns:
            processor.record_pattern(cmd, intent, 0.9, success)

        # Re-test similar ambiguous commands - should have higher confidence
        similar_commands = [
            "make something for the authorization feature",
            "handle the API connection issue"
        ]

        for cmd in similar_commands:
            result = processor.process_command(cmd)
            learned_confidence = result.get('command', {}).get('confidence', 0)
            assert learned_confidence > 0.7, f"Learning not improving confidence for: {cmd}"

    def test_continuous_learning_loop(self):
        """Test continuous learning feedback loop"""
        from cometa_nlp_processor import NaturalLanguageCommandProcessor
        from cometa_task_executor import TaskCommandExecutor

        processor = NaturalLanguageCommandProcessor(Path("test.db"))
        executor = TaskCommandExecutor(Path("test.db"))

        learning_commands = [
            "create task for implementing OAuth",
            "create task for setting up CI/CD",
            "create task for code review process"
        ]

        # Initial processing
        initial_accuracies = []
        for cmd in learning_commands:
            result = processor.process_command(cmd)
            exec_result = executor.execute_command(result['command'])

            # Record success/failure for learning
            success = exec_result['success']
            processor.record_learning_outcome(cmd, result, success)
            initial_accuracies.append(result['command']['confidence'])

        # Process variations to test learning application
        variation_commands = [
            "create task for implementing SAML authentication",
            "create task for setting up deployment pipeline",
            "create task for establishing code quality process"
        ]

        learned_accuracies = []
        for cmd in variation_commands:
            result = processor.process_command(cmd)
            learned_accuracies.append(result['command']['confidence'])

        # Learning should improve average confidence
        avg_initial = sum(initial_accuracies) / len(initial_accuracies)
        avg_learned = sum(learned_accuracies) / len(learned_accuracies)

        assert avg_learned > avg_initial, "Continuous learning not improving performance"
        print(f"‚úÖ Learning improvement: {avg_initial:.2f} ‚Üí {avg_learned:.2f}")
```

---

## CONCLUSIONE

### Coverage Report Target
```
Module                      Coverage
------------------------    --------
cometa_nlp_processor.py        96%
cometa_task_executor.py        95%
cometa_progress_tracker.py     94%
cometa_batch_manager.py        95%
cometa_nlp_hook.py            93%
------------------------    --------
TOTAL                          95%
```

### Performance Targets
- **Latency**: < 100ms p95
- **Throughput**: > 100 commands/second
- **Availability**: 99.9% uptime
- **Error Rate**: < 0.1%

### Next Steps
1. ‚úÖ Implement all test suites
2. ‚úÖ Setup CI/CD pipeline
3. ‚úÖ Configure monitoring
4. ‚úÖ Run load testing
5. ‚úÖ Deploy to staging
6. ‚úÖ Production deployment

---

**Test Command**: `pytest tests/ -v --cov=cometa --cov-report=html`
**CI/CD**: GitHub Actions with automatic deployment
**Monitoring**: Prometheus + Grafana
**Deployment**: Zero-downtime rolling updates