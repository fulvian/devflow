#!/usr/bin/env python3
"""
Enhanced Context7 Quality Metrics System
Multi-dimensional quality assessment based on Easystats Performance patterns

Implementa sistema di valutazione qualitÃ  multi-dimensionale per Context7
usando pattern statistici validati da Easystats Performance library.

Features:
- Multi-dimensional quality metrics (AIC, BIC, RÂ²-adjusted, RMSE, ICC)
- Statistical validation con confidence intervals
- Cross-validation performance assessment
- Composite quality scoring con weighted aggregation
- Performance monitoring integration ready
"""

import json
import sqlite3
import numpy as np
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from scipy import stats
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, KFold

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """Structured quality metrics container"""
    # Core performance metrics (Easystats-inspired)
    aic: float  # Akaike Information Criterion
    bic: float  # Bayesian Information Criterion
    r2_adjusted: float  # Adjusted R-squared
    rmse: float  # Root Mean Squared Error
    mae: float  # Mean Absolute Error
    icc: float  # Intraclass Correlation Coefficient

    # Context7-specific metrics
    similarity_score: float  # Vector similarity accuracy
    semantic_coherence: float  # Semantic relevance score
    retrieval_precision: float  # Precision of context retrieval
    processing_efficiency: float  # Speed/resource efficiency

    # Meta-metrics
    composite_score: float  # Weighted composite quality score
    confidence_interval: Tuple[float, float]  # 95% CI for composite
    statistical_significance: float  # p-value for quality assessment
    cross_validation_score: float  # CV performance

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'aic': self.aic,
            'bic': self.bic,
            'r2_adjusted': self.r2_adjusted,
            'rmse': self.rmse,
            'mae': self.mae,
            'icc': self.icc,
            'similarity_score': self.similarity_score,
            'semantic_coherence': self.semantic_coherence,
            'retrieval_precision': self.retrieval_precision,
            'processing_efficiency': self.processing_efficiency,
            'composite_score': self.composite_score,
            'confidence_interval': list(self.confidence_interval),
            'statistical_significance': self.statistical_significance,
            'cross_validation_score': self.cross_validation_score
        }

class EnhancedQualityMetricsCalculator:
    """
    Enhanced quality metrics calculator per Context7
    Basato su pattern Easystats Performance per statistical validation
    """

    def __init__(self, project_root: str = "/Users/fulvioventura/devflow"):
        self.project_root = Path(project_root)
        self.db_path = self.project_root / "data" / "devflow_unified.sqlite"

        # Metric weights (tunable based on Context7 requirements)
        self.weights = {
            'aic': 0.1,
            'bic': 0.1,
            'r2_adjusted': 0.15,
            'rmse': 0.15,
            'mae': 0.1,
            'icc': 0.1,
            'similarity_score': 0.2,  # High weight per Context7
            'semantic_coherence': 0.15,
            'retrieval_precision': 0.15,
            'processing_efficiency': 0.1
        }

        # Quality thresholds (Context7 Full Mode requirements)
        self.thresholds = {
            'minimum_composite': 0.75,  # Required for Full Mode
            'excellent_composite': 0.90,
            'statistical_significance': 0.05  # p < 0.05
        }

    def calculate_aic_bic(self, residuals: np.ndarray, n_params: int) -> Tuple[float, float]:
        """
        Calculate AIC and BIC metrics
        Based on Easystats Performance model_performance patterns
        """
        n = len(residuals)
        mse = np.mean(residuals ** 2)
        log_likelihood = -0.5 * n * (np.log(2 * np.pi) + np.log(mse) + 1)

        # AIC = -2 * log_likelihood + 2 * n_params
        aic = -2 * log_likelihood + 2 * n_params

        # BIC = -2 * log_likelihood + ln(n) * n_params
        bic = -2 * log_likelihood + np.log(n) * n_params

        return aic, bic

    def calculate_r2_adjusted(self, y_true: np.ndarray, y_pred: np.ndarray, n_params: int) -> float:
        """
        Calculate adjusted R-squared
        Following Easystats Performance r2() patterns con penalty per complexity
        """
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)

        if ss_tot == 0:
            return 0.0

        r2 = 1 - (ss_res / ss_tot)
        n = len(y_true)

        # Adjusted RÂ² = 1 - (1 - RÂ²) * (n - 1) / (n - p - 1)
        r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - n_params - 1)

        return max(0.0, r2_adjusted)  # Ensure non-negative

    def calculate_icc(self, measurements: List[List[float]]) -> float:
        """
        Calculate Intraclass Correlation Coefficient
        Based on Easystats Performance icc() patterns
        """
        if not measurements or len(measurements) < 2:
            return 0.0

        # Convert to numpy array
        data = np.array(measurements)
        n_subjects, n_raters = data.shape

        # Calculate ICC(2,1) - two-way random effects, single measures
        # Following Shrout & Fleiss (1979) formulation
        subject_means = np.mean(data, axis=1)
        grand_mean = np.mean(data)

        # Between-subjects sum of squares
        ss_between = n_raters * np.sum((subject_means - grand_mean) ** 2)

        # Within-subjects sum of squares
        ss_within = np.sum((data - subject_means.reshape(-1, 1)) ** 2)

        # Total sum of squares
        ss_total = np.sum((data - grand_mean) ** 2)

        # Mean squares
        ms_between = ss_between / (n_subjects - 1)
        ms_within = ss_within / (n_subjects * (n_raters - 1))

        # ICC calculation
        if ms_within == 0:
            return 1.0

        icc = (ms_between - ms_within) / (ms_between + (n_raters - 1) * ms_within)

        return max(0.0, min(1.0, icc))  # Clamp to [0,1]

    def calculate_semantic_coherence(self, contexts: List[Dict], query: str) -> float:
        """
        Calculate semantic coherence score
        Measures how well retrieved contexts relate to each other and the query
        """
        if not contexts:
            return 0.0

        # Placeholder for semantic coherence calculation
        # In production, would use embedding similarity matrix
        coherence_scores = []

        for context in contexts:
            # Simplified coherence based on content overlap
            content = context.get('content', '')
            query_words = set(query.lower().split())
            content_words = set(content.lower().split())

            if not query_words or not content_words:
                coherence = 0.0
            else:
                overlap = len(query_words.intersection(content_words))
                coherence = overlap / len(query_words.union(content_words))

            coherence_scores.append(coherence)

        return np.mean(coherence_scores) if coherence_scores else 0.0

    def calculate_retrieval_precision(self, retrieved_contexts: List[Dict],
                                    relevant_context_ids: set) -> float:
        """
        Calculate precision of context retrieval
        Precision = |relevant âˆ© retrieved| / |retrieved|
        """
        if not retrieved_contexts:
            return 0.0

        retrieved_ids = {ctx.get('id', '') for ctx in retrieved_contexts}
        relevant_retrieved = retrieved_ids.intersection(relevant_context_ids)

        return len(relevant_retrieved) / len(retrieved_ids)

    def calculate_processing_efficiency(self, processing_time_ms: float,
                                      context_count: int) -> float:
        """
        Calculate processing efficiency score
        Normalized efficiency based on time per context
        """
        if processing_time_ms <= 0 or context_count <= 0:
            return 0.0

        # Time per context in milliseconds
        time_per_context = processing_time_ms / context_count

        # Efficiency score (inverse relationship with time)
        # 100ms per context = 0.5 score, 50ms = 0.75, 25ms = 0.9
        efficiency = 1 / (1 + time_per_context / 50)

        return min(1.0, efficiency)

    def calculate_cross_validation_score(self, X: np.ndarray, y: np.ndarray,
                                       model_func, cv_folds: int = 5) -> float:
        """
        Calculate cross-validation performance score
        Following Easystats Performance performance_cv() patterns
        """
        try:
            kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
            cv_scores = cross_val_score(model_func, X, y, cv=kfold, scoring='r2')
            return np.mean(cv_scores)
        except Exception as e:
            logger.warning(f"Cross-validation failed: {e}")
            return 0.0

    def calculate_composite_score(self, individual_metrics: Dict[str, float]) -> float:
        """
        Calculate weighted composite quality score
        Following multi-dimensional aggregation patterns
        """
        weighted_sum = 0.0
        total_weight = 0.0

        for metric, value in individual_metrics.items():
            if metric in self.weights:
                weight = self.weights[metric]
                weighted_sum += value * weight
                total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0.0

    def calculate_confidence_interval(self, scores: List[float],
                                    confidence_level: float = 0.95) -> Tuple[float, float]:
        """
        Calculate confidence interval for quality scores
        Using bootstrap method similar to Easystats Performance
        """
        if not scores:
            return (0.0, 0.0)

        scores_array = np.array(scores)
        alpha = 1 - confidence_level

        # Bootstrap confidence interval
        n_bootstrap = 1000
        bootstrap_means = []

        for _ in range(n_bootstrap):
            sample = np.random.choice(scores_array, size=len(scores_array), replace=True)
            bootstrap_means.append(np.mean(sample))

        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100

        ci_lower = np.percentile(bootstrap_means, lower_percentile)
        ci_upper = np.percentile(bootstrap_means, upper_percentile)

        return (ci_lower, ci_upper)

    def perform_statistical_test(self, quality_scores: List[float],
                                baseline_threshold: float = 0.75) -> float:
        """
        Perform statistical significance test
        H0: quality_score <= baseline_threshold
        H1: quality_score > baseline_threshold
        """
        if not quality_scores:
            return 1.0  # No significance

        scores_array = np.array(quality_scores)

        # One-sample t-test against baseline threshold
        t_stat, p_value = stats.ttest_1samp(scores_array, baseline_threshold)

        # One-tailed test (we want scores > threshold)
        if t_stat > 0:
            p_value = p_value / 2
        else:
            p_value = 1 - (p_value / 2)

        return p_value

    def assess_context7_quality(self, context_data: Dict[str, Any]) -> QualityMetrics:
        """
        Main quality assessment function per Context7
        Integrates tutti i metrics in comprehensive evaluation
        """
        # Extract data from context
        residuals = np.array(context_data.get('residuals', [0.1] * 10))
        y_true = np.array(context_data.get('y_true', list(range(10))))
        y_pred = np.array(context_data.get('y_pred', list(range(10))))
        contexts = context_data.get('retrieved_contexts', [])
        query = context_data.get('query', '')
        processing_time = context_data.get('processing_time_ms', 100)
        n_params = context_data.get('n_parameters', 5)

        # Calculate individual metrics
        aic, bic = self.calculate_aic_bic(residuals, n_params)
        r2_adj = self.calculate_r2_adjusted(y_true, y_pred, n_params)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)

        # Placeholder ICC calculation (would need proper measurement data)
        measurements = context_data.get('measurements', [[0.8, 0.7], [0.9, 0.8]])
        icc = self.calculate_icc(measurements)

        # Context7-specific metrics
        similarity_score = context_data.get('similarity_score', 0.8)
        semantic_coherence = self.calculate_semantic_coherence(contexts, query)

        # Placeholder precision (would need ground truth)
        relevant_ids = set(context_data.get('relevant_context_ids', ['1', '2']))
        retrieval_precision = self.calculate_retrieval_precision(contexts, relevant_ids)

        processing_efficiency = self.calculate_processing_efficiency(
            processing_time, len(contexts)
        )

        # Aggregate metrics
        individual_metrics = {
            'aic': 1 / (1 + aic / 1000),  # Normalize AIC (lower is better)
            'bic': 1 / (1 + bic / 1000),  # Normalize BIC (lower is better)
            'r2_adjusted': r2_adj,
            'rmse': 1 / (1 + rmse),  # Normalize RMSE (lower is better)
            'mae': 1 / (1 + mae),    # Normalize MAE (lower is better)
            'icc': icc,
            'similarity_score': similarity_score,
            'semantic_coherence': semantic_coherence,
            'retrieval_precision': retrieval_precision,
            'processing_efficiency': processing_efficiency
        }

        composite_score = self.calculate_composite_score(individual_metrics)

        # Statistical validation
        quality_history = context_data.get('quality_history', [composite_score] * 10)
        ci = self.calculate_confidence_interval(quality_history)
        p_value = self.perform_statistical_test(quality_history)

        # Cross-validation (placeholder)
        cv_score = context_data.get('cv_score', composite_score * 0.95)

        return QualityMetrics(
            aic=aic,
            bic=bic,
            r2_adjusted=r2_adj,
            rmse=rmse,
            mae=mae,
            icc=icc,
            similarity_score=similarity_score,
            semantic_coherence=semantic_coherence,
            retrieval_precision=retrieval_precision,
            processing_efficiency=processing_efficiency,
            composite_score=composite_score,
            confidence_interval=ci,
            statistical_significance=p_value,
            cross_validation_score=cv_score
        )

    def evaluate_full_mode_readiness(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """
        Evaluate readiness for Context7 Full Mode transition
        Based on quality thresholds and statistical validation
        """
        readiness_check = {
            'ready_for_full_mode': False,
            'quality_assessment': 'insufficient',
            'recommendations': [],
            'blocking_issues': [],
            'confidence_level': 'low'
        }

        # Check composite score threshold
        if metrics.composite_score >= self.thresholds['minimum_composite']:
            readiness_check['ready_for_full_mode'] = True
            readiness_check['quality_assessment'] = 'sufficient'
        else:
            readiness_check['blocking_issues'].append(
                f"Composite score {metrics.composite_score:.3f} below threshold {self.thresholds['minimum_composite']}"
            )

        # Check statistical significance
        if metrics.statistical_significance > self.thresholds['statistical_significance']:
            readiness_check['blocking_issues'].append(
                f"Quality improvement not statistically significant (p={metrics.statistical_significance:.3f})"
            )

        # Check individual metrics
        if metrics.similarity_score < 0.7:
            readiness_check['recommendations'].append("Improve vector similarity algorithms")

        if metrics.semantic_coherence < 0.6:
            readiness_check['recommendations'].append("Enhance semantic coherence evaluation")

        if metrics.retrieval_precision < 0.7:
            readiness_check['recommendations'].append("Optimize context retrieval precision")

        # Set confidence level
        ci_width = metrics.confidence_interval[1] - metrics.confidence_interval[0]
        if ci_width < 0.1:
            readiness_check['confidence_level'] = 'high'
        elif ci_width < 0.2:
            readiness_check['confidence_level'] = 'medium'

        # Excellence check
        if metrics.composite_score >= self.thresholds['excellent_composite']:
            readiness_check['quality_assessment'] = 'excellent'

        return readiness_check

    def save_metrics_to_database(self, metrics: QualityMetrics,
                               session_id: str = None) -> None:
        """
        Save quality metrics to unified database
        For monitoring and historical analysis
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # Insert into context7_quality_metrics table (create if not exists)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS context7_quality_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT,
                    composite_score REAL,
                    aic REAL,
                    bic REAL,
                    r2_adjusted REAL,
                    rmse REAL,
                    mae REAL,
                    icc REAL,
                    similarity_score REAL,
                    semantic_coherence REAL,
                    retrieval_precision REAL,
                    processing_efficiency REAL,
                    confidence_interval_lower REAL,
                    confidence_interval_upper REAL,
                    statistical_significance REAL,
                    cross_validation_score REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            cursor.execute("""
                INSERT INTO context7_quality_metrics (
                    session_id, composite_score, aic, bic, r2_adjusted, rmse, mae, icc,
                    similarity_score, semantic_coherence, retrieval_precision,
                    processing_efficiency, confidence_interval_lower, confidence_interval_upper,
                    statistical_significance, cross_validation_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                session_id or f"session_{datetime.now().timestamp()}",
                metrics.composite_score, metrics.aic, metrics.bic, metrics.r2_adjusted,
                metrics.rmse, metrics.mae, metrics.icc, metrics.similarity_score,
                metrics.semantic_coherence, metrics.retrieval_precision,
                metrics.processing_efficiency, metrics.confidence_interval[0],
                metrics.confidence_interval[1], metrics.statistical_significance,
                metrics.cross_validation_score
            ))

            conn.commit()
            conn.close()

            logger.info(f"Quality metrics saved: composite_score={metrics.composite_score:.3f}")

        except Exception as e:
            logger.error(f"Failed to save metrics to database: {e}")

def main():
    """Test the enhanced quality metrics system"""
    calculator = EnhancedQualityMetricsCalculator()

    # Test data simulation
    test_context_data = {
        'residuals': np.random.normal(0, 0.1, 100).tolist(),
        'y_true': np.random.normal(10, 2, 100).tolist(),
        'y_pred': (np.random.normal(10, 2, 100) + np.random.normal(0, 0.2, 100)).tolist(),
        'retrieved_contexts': [
            {'id': '1', 'content': 'test context about machine learning optimization'},
            {'id': '2', 'content': 'context about performance metrics validation'},
            {'id': '3', 'content': 'statistical analysis and quality assessment'}
        ],
        'query': 'machine learning performance optimization',
        'processing_time_ms': 85,
        'n_parameters': 8,
        'similarity_score': 0.82,
        'relevant_context_ids': ['1', '2'],
        'quality_history': [0.78, 0.81, 0.79, 0.83, 0.80, 0.82, 0.84, 0.79, 0.81, 0.83],
        'cv_score': 0.79
    }

    # Calculate quality metrics
    metrics = calculator.assess_context7_quality(test_context_data)

    # Evaluate readiness
    readiness = calculator.evaluate_full_mode_readiness(metrics)

    # Save to database
    calculator.save_metrics_to_database(metrics, "enhanced_quality_test")

    # Output results
    print("ðŸŽ¯ Enhanced Context7 Quality Assessment Results")
    print("=" * 60)
    print(f"Composite Score: {metrics.composite_score:.3f}")
    print(f"Confidence Interval: [{metrics.confidence_interval[0]:.3f}, {metrics.confidence_interval[1]:.3f}]")
    print(f"Statistical Significance: p={metrics.statistical_significance:.3f}")
    print(f"Cross-validation Score: {metrics.cross_validation_score:.3f}")
    print()
    print("Individual Metrics:")
    print(f"  RÂ² Adjusted: {metrics.r2_adjusted:.3f}")
    print(f"  RMSE: {metrics.rmse:.3f}")
    print(f"  Similarity Score: {metrics.similarity_score:.3f}")
    print(f"  Semantic Coherence: {metrics.semantic_coherence:.3f}")
    print(f"  Retrieval Precision: {metrics.retrieval_precision:.3f}")
    print()
    print("Full Mode Readiness:")
    print(f"  Ready: {readiness['ready_for_full_mode']}")
    print(f"  Assessment: {readiness['quality_assessment']}")
    print(f"  Confidence: {readiness['confidence_level']}")

    if readiness['blocking_issues']:
        print("  Blocking Issues:")
        for issue in readiness['blocking_issues']:
            print(f"    - {issue}")

    if readiness['recommendations']:
        print("  Recommendations:")
        for rec in readiness['recommendations']:
            print(f"    - {rec}")

if __name__ == "__main__":
    main()